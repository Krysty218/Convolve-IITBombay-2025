{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('/home/kaeshav/Documents/Convolve/Dev_data_to_be_shared.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 96806 entries, 0 to 96805\n",
      "Columns: 1216 entries, account_number to onus_attribute_48\n",
      "dtypes: float64(1189), int64(27)\n",
      "memory usage: 898.1 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_number</th>\n",
       "      <th>bad_flag</th>\n",
       "      <th>onus_attribute_1</th>\n",
       "      <th>transaction_attribute_1</th>\n",
       "      <th>transaction_attribute_2</th>\n",
       "      <th>transaction_attribute_3</th>\n",
       "      <th>transaction_attribute_4</th>\n",
       "      <th>transaction_attribute_5</th>\n",
       "      <th>transaction_attribute_6</th>\n",
       "      <th>transaction_attribute_7</th>\n",
       "      <th>...</th>\n",
       "      <th>bureau_enquiry_47</th>\n",
       "      <th>bureau_enquiry_48</th>\n",
       "      <th>bureau_enquiry_49</th>\n",
       "      <th>bureau_enquiry_50</th>\n",
       "      <th>onus_attribute_43</th>\n",
       "      <th>onus_attribute_44</th>\n",
       "      <th>onus_attribute_45</th>\n",
       "      <th>onus_attribute_46</th>\n",
       "      <th>onus_attribute_47</th>\n",
       "      <th>onus_attribute_48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>221000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>86000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>215000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   account_number  bad_flag  onus_attribute_1  transaction_attribute_1  \\\n",
       "0               1         0               NaN                      NaN   \n",
       "1               2         0          221000.0                      0.0   \n",
       "2               3         0           25000.0                      0.0   \n",
       "3               4         0           86000.0                      0.0   \n",
       "4               5         0          215000.0                      0.0   \n",
       "\n",
       "   transaction_attribute_2  transaction_attribute_3  transaction_attribute_4  \\\n",
       "0                      NaN                      NaN                      NaN   \n",
       "1                      0.0                      0.0                      0.0   \n",
       "2                      0.0                      0.0                      0.0   \n",
       "3                      0.0                      0.0                      0.0   \n",
       "4                      0.0                      0.0                      0.0   \n",
       "\n",
       "   transaction_attribute_5  transaction_attribute_6  transaction_attribute_7  \\\n",
       "0                      NaN                      NaN                      NaN   \n",
       "1                      0.0                      0.0                      0.0   \n",
       "2                      0.0                      0.0                      0.0   \n",
       "3                      0.0                      0.0                      0.0   \n",
       "4                      0.0                      0.0                      0.0   \n",
       "\n",
       "   ...  bureau_enquiry_47  bureau_enquiry_48  bureau_enquiry_49  \\\n",
       "0  ...                0.0                0.0                0.0   \n",
       "1  ...                0.0                0.0                2.0   \n",
       "2  ...                0.0                0.0                0.0   \n",
       "3  ...                0.0                0.0                0.0   \n",
       "4  ...                0.0                0.0                0.0   \n",
       "\n",
       "   bureau_enquiry_50  onus_attribute_43  onus_attribute_44  onus_attribute_45  \\\n",
       "0                1.0                NaN                NaN                NaN   \n",
       "1                3.0                0.0                0.0                0.0   \n",
       "2                8.0                NaN                NaN                NaN   \n",
       "3               30.0                NaN                NaN                NaN   \n",
       "4                1.0                NaN                NaN                NaN   \n",
       "\n",
       "   onus_attribute_46  onus_attribute_47  onus_attribute_48  \n",
       "0                NaN                NaN                NaN  \n",
       "1                0.0                0.0                0.0  \n",
       "2                NaN                NaN                NaN  \n",
       "3                NaN                NaN                NaN  \n",
       "4                NaN                NaN                NaN  \n",
       "\n",
       "[5 rows x 1216 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info()\n",
    "df.describe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df1.drop(['bad_flag'],axis=1)\n",
    "X=X.drop(['account_number'],axis=1)\n",
    "y=df1['bad_flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['onus_attribute_1', 'transaction_attribute_1',\n",
      "       'transaction_attribute_2', 'transaction_attribute_3',\n",
      "       'transaction_attribute_4', 'transaction_attribute_5',\n",
      "       'transaction_attribute_6', 'transaction_attribute_7',\n",
      "       'transaction_attribute_8', 'transaction_attribute_9',\n",
      "       ...\n",
      "       'bureau_enquiry_47', 'bureau_enquiry_48', 'bureau_enquiry_49',\n",
      "       'bureau_enquiry_50', 'onus_attribute_43', 'onus_attribute_44',\n",
      "       'onus_attribute_45', 'onus_attribute_46', 'onus_attribute_47',\n",
      "       'onus_attribute_48'],\n",
      "      dtype='object', length=1214)\n"
     ]
    }
   ],
   "source": [
    "numeric_features=X.select_dtypes(include=['int64','float']).columns\n",
    "print(numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report, roc_auc_score, log_loss\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "X_scaled=scaler.fit_transform(X)\n",
    "y=y.values.reshape(-1,1)\n",
    "y_scaled=scaler.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 98.45057328788349\n",
      "[[19062   278]\n",
      " [   22     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     19340\n",
      "           1       0.00      0.00      0.00        22\n",
      "\n",
      "    accuracy                           0.98     19362\n",
      "   macro avg       0.50      0.49      0.50     19362\n",
      "weighted avg       1.00      0.98      0.99     19362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=LogisticRegressionCV()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "print('Accuracy score:', accuracy_score(y_pred,y_test)*100)\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 98.55903315773163\n",
      "[[19083   278]\n",
      " [    1     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     19361\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.99     19362\n",
      "   macro avg       0.50      0.49      0.50     19362\n",
      "weighted avg       1.00      0.99      0.99     19362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model1=XGBClassifier()\n",
    "model1.fit(X_train,y_train)\n",
    "y_pred1=model1.predict(X_test)\n",
    "print('Accuracy score:', accuracy_score(y_pred1,y_test)*100)\n",
    "print(confusion_matrix(y_pred1,y_test))\n",
    "print(classification_report(y_pred1,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 98.55903315773163\n",
      "AUC-ROC: 0.8010109414040765\n",
      "Log Loss: 0.6255704741035657\n",
      "Gini Coefficient: 0.6020218828081529\n",
      "[[19081   276]\n",
      " [    3     2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     19357\n",
      "           1       0.01      0.40      0.01         5\n",
      "\n",
      "    accuracy                           0.99     19362\n",
      "   macro avg       0.50      0.69      0.50     19362\n",
      "weighted avg       1.00      0.99      0.99     19362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2=AdaBoostClassifier()\n",
    "model2.fit(X_train,y_train)\n",
    "y_pred2=model2.predict(X_test)\n",
    "print('Accuracy score:', accuracy_score(y_pred2,y_test)*100)\n",
    "y_prob = model2.predict_proba(X_test)[:, 1]  \n",
    "# Metrics\n",
    "#accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "logloss = log_loss(y_test, y_prob)\n",
    "#f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "gini = 2 * roc_auc - 1  # Gini coefficient from AUC\n",
    "\n",
    "print(\"AUC-ROC:\",roc_auc)\n",
    "print(\"Log Loss:\",logloss)\n",
    "print(\"Gini Coefficient:\", gini)\n",
    "print(confusion_matrix(y_pred2,y_test))\n",
    "print(classification_report(y_pred2,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 98.56419791343869\n",
      "[[19084   278]\n",
      " [    0     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     19362\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99     19362\n",
      "   macro avg       0.50      0.49      0.50     19362\n",
      "weighted avg       1.00      0.99      0.99     19362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model3 = RandomForestClassifier(n_estimators=100, random_state=42)  \n",
    "model3.fit(X_train, y_train)\n",
    "y_pred3 = model3.predict(X_test)\n",
    "print('Accuracy score:', accuracy_score(y_pred3, y_test) * 100)\n",
    "print(confusion_matrix(y_pred3, y_test))\n",
    "print(classification_report(y_pred3, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Validation** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = pd.read_csv('/home/kaeshav/Documents/Convolve/validation_data_to_be_shared.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_filled = validation_data.fillna(0)\n",
    "validation_data_filled = validation_data_filled.drop([\"account_number\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['onus_attribute_1', 'transaction_attribute_1',\n",
       "       'transaction_attribute_2', 'transaction_attribute_3',\n",
       "       'transaction_attribute_4', 'transaction_attribute_5',\n",
       "       'transaction_attribute_6', 'transaction_attribute_7',\n",
       "       'transaction_attribute_8', 'transaction_attribute_9',\n",
       "       ...\n",
       "       'bureau_enquiry_47', 'bureau_enquiry_48', 'bureau_enquiry_49',\n",
       "       'bureau_enquiry_50', 'onus_attribute_43', 'onus_attribute_44',\n",
       "       'onus_attribute_45', 'onus_attribute_46', 'onus_attribute_47',\n",
       "       'onus_attribute_48'],\n",
       "      dtype='object', length=1214)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_features=validation_data_filled.select_dtypes(include=['int64','float']).columns\n",
    "numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation = validation_data_filled[numeric_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "X_scaled=scaler.fit_transform(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation_scaled = scaler.transform(X_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LogisticRegressionCV** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for validation data saved to 'submission_LCV.csv'\n"
     ]
    }
   ],
   "source": [
    "validation_data['predicted_probability'] = model.predict(X_validation_scaled)  # For the Random Forest\n",
    "submission = validation_data[['account_number', 'predicted_probability']]\n",
    "submission.to_csv('submission_LCV.csv', index=False)\n",
    "\n",
    "print(\"Prediction for validation data saved to 'submission_LCV.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for validation data saved to 'submission_XGB.csv'\n"
     ]
    }
   ],
   "source": [
    "validation_data['predicted_probability'] = model1.predict(X_validation_scaled)  # For the Random Forest\n",
    "submission = validation_data[['account_number', 'predicted_probability']]\n",
    "submission.to_csv('submission_XGB.csv', index=False)\n",
    "\n",
    "print(\"Prediction for validation data saved to 'submission_XGB.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AdaBoost** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for validation data saved to 'submission_Ada.csv'\n"
     ]
    }
   ],
   "source": [
    "validation_data['predicted_probability'] = model2.predict(X_validation_scaled)  # For the Random Forest\n",
    "submission = validation_data[['account_number', 'predicted_probability']]\n",
    "submission.to_csv('submission_Ada.csv', index=False)\n",
    "\n",
    "print(\"Prediction for validation data saved to 'submission_Ada.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Random Forest** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for validation data saved to 'submission_RF.csv'\n"
     ]
    }
   ],
   "source": [
    "validation_data['predicted_probability'] = model3.predict(X_validation_scaled)  # For the Random Forest\n",
    "submission = validation_data[['account_number', 'predicted_probability']]\n",
    "submission.to_csv('submission_RF.csv', index=False)\n",
    "\n",
    "print(\"Prediction for validation data saved to 'submission_RF.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Neural Network** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9709 - loss: 0.1144 - val_accuracy: 0.9856 - val_loss: 0.0708\n",
      "Epoch 2/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 716us/step - accuracy: 0.9863 - loss: 0.0709 - val_accuracy: 0.9856 - val_loss: 0.0690\n",
      "Epoch 3/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709us/step - accuracy: 0.9862 - loss: 0.0671 - val_accuracy: 0.9856 - val_loss: 0.0677\n",
      "Epoch 4/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 732us/step - accuracy: 0.9854 - loss: 0.0672 - val_accuracy: 0.9856 - val_loss: 0.0688\n",
      "Epoch 5/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 718us/step - accuracy: 0.9855 - loss: 0.0628 - val_accuracy: 0.9856 - val_loss: 0.0666\n",
      "Epoch 6/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 750us/step - accuracy: 0.9860 - loss: 0.0616 - val_accuracy: 0.9856 - val_loss: 0.0677\n",
      "Epoch 7/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 738us/step - accuracy: 0.9855 - loss: 0.0611 - val_accuracy: 0.9856 - val_loss: 0.0714\n",
      "Epoch 8/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 744us/step - accuracy: 0.9858 - loss: 0.0579 - val_accuracy: 0.9856 - val_loss: 0.0687\n",
      "Epoch 9/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 736us/step - accuracy: 0.9860 - loss: 0.0574 - val_accuracy: 0.9856 - val_loss: 0.0718\n",
      "Epoch 10/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 727us/step - accuracy: 0.9864 - loss: 0.0534 - val_accuracy: 0.9856 - val_loss: 0.0703\n",
      "Epoch 11/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 729us/step - accuracy: 0.9860 - loss: 0.0544 - val_accuracy: 0.9856 - val_loss: 0.0743\n",
      "Epoch 12/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 741us/step - accuracy: 0.9855 - loss: 0.0532 - val_accuracy: 0.9856 - val_loss: 0.0782\n",
      "Epoch 13/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 804us/step - accuracy: 0.9862 - loss: 0.0501 - val_accuracy: 0.9855 - val_loss: 0.0763\n",
      "Epoch 14/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 814us/step - accuracy: 0.9859 - loss: 0.0499 - val_accuracy: 0.9856 - val_loss: 0.0778\n",
      "Epoch 15/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 816us/step - accuracy: 0.9866 - loss: 0.0475 - val_accuracy: 0.9855 - val_loss: 0.0846\n",
      "Epoch 16/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 752us/step - accuracy: 0.9862 - loss: 0.0488 - val_accuracy: 0.9855 - val_loss: 0.0820\n",
      "Epoch 17/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 723us/step - accuracy: 0.9870 - loss: 0.0444 - val_accuracy: 0.9856 - val_loss: 0.0962\n",
      "Epoch 18/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 757us/step - accuracy: 0.9868 - loss: 0.0444 - val_accuracy: 0.9855 - val_loss: 0.0959\n",
      "Epoch 19/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 740us/step - accuracy: 0.9871 - loss: 0.0418 - val_accuracy: 0.9853 - val_loss: 0.0933\n",
      "Epoch 20/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 745us/step - accuracy: 0.9874 - loss: 0.0432 - val_accuracy: 0.9851 - val_loss: 0.0994\n",
      "Epoch 21/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 738us/step - accuracy: 0.9884 - loss: 0.0388 - val_accuracy: 0.9851 - val_loss: 0.0969\n",
      "Epoch 22/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 752us/step - accuracy: 0.9875 - loss: 0.0413 - val_accuracy: 0.9852 - val_loss: 0.0969\n",
      "Epoch 23/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 739us/step - accuracy: 0.9885 - loss: 0.0380 - val_accuracy: 0.9854 - val_loss: 0.1068\n",
      "Epoch 24/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 746us/step - accuracy: 0.9881 - loss: 0.0374 - val_accuracy: 0.9855 - val_loss: 0.1135\n",
      "Epoch 25/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 752us/step - accuracy: 0.9884 - loss: 0.0373 - val_accuracy: 0.9848 - val_loss: 0.1099\n",
      "Epoch 26/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 743us/step - accuracy: 0.9885 - loss: 0.0376 - val_accuracy: 0.9853 - val_loss: 0.1196\n",
      "Epoch 27/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 747us/step - accuracy: 0.9884 - loss: 0.0359 - val_accuracy: 0.9853 - val_loss: 0.1100\n",
      "Epoch 28/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 747us/step - accuracy: 0.9890 - loss: 0.0343 - val_accuracy: 0.9853 - val_loss: 0.1393\n",
      "Epoch 29/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 727us/step - accuracy: 0.9901 - loss: 0.0326 - val_accuracy: 0.9851 - val_loss: 0.1244\n",
      "Epoch 30/30\n",
      "\u001b[1m1211/1211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 747us/step - accuracy: 0.9904 - loss: 0.0319 - val_accuracy: 0.9852 - val_loss: 0.1447\n",
      "\u001b[1m606/606\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 663us/step\n",
      "Accuracy: 0.9852287986778225\n",
      "AUC-ROC: 0.7156060521526187\n",
      "Log Loss: 0.12044939939282596\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=1)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Build the neural network model\n",
    "model4 = Sequential([\n",
    "    Dense(128, input_dim=X_train.shape[1], activation='relu'),  # Input layer\n",
    "    Dropout(0.4), \n",
    "    Dense(128, activation='relu'),  # Hidden layer\n",
    "    Dropout(0.4),\n",
    "    Dense(1, activation='sigmoid')  # Output layer \n",
    "\n",
    "# Compile the model\n",
    "model4.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model4.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=64, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_proba = model4.predict(X_test).flatten()  # Predicted probabilities\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "logloss = log_loss(y_test, y_pred_proba)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"AUC-ROC:\", roc_auc)\n",
    "print(\"Log Loss:\", logloss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1306/1306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 473us/step\n",
      "Prediction for validation data saved to 'submission_NN.csv'\n"
     ]
    }
   ],
   "source": [
    "predicted_probability = model4.predict(X_validation_scaled).flatten()\n",
    "\n",
    "predicted_class = (predicted_probability > 0.5).astype(int)\n",
    "\n",
    "validation_data['predicted_class'] = predicted_class\n",
    "\n",
    "submission = validation_data[['account_number', 'predicted_class']]\n",
    "\n",
    "submission.to_csv('submission_NN.csv', index=False)\n",
    "print(\"Prediction for validation data saved to 'submission_NN.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "con",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
